#                   ================================================================
"""                    Simple Agent with Gemini and litellm, Asyncio & Chainlit ğŸ¤–ğŸ”—âœ¨          """
#                   ================================================================
# ğŸ“ Short Note:
#              A lightweight AI agent built using the Gemini API and
#              Python's asyncio, powered by Chainlit to create and run multiple agents,
#              including integrations with OpenAI. ğŸ”‘ğŸŒ€ğŸ’¡ğŸ¤



#               =====================
#"""                 ğŸ“Œ  Step 1ï¸                             """
#               =====================


# from agents import Agent, Runner, set_tracing_disabled, OpenAIChatCompletionsModel
# from dotenv import load_dotenv
# import os
# from openai import AsyncOpenAI  # Ensure this is the correct module for AsyncOpenAI
# #from chainlit import cl  # Ensure you have the correct import for your chat library

# import chainlit as cl



# # Load environment variables from .env file
# load_dotenv()

# client = AsyncOpenAI(
#     api_key=os.getenv("OpenRouter_API_KEY"),
#     base_url="https://openrouter.ai/api/v1",  
#     # Ensure this is the correct base URL for the OpenRouter API
#     # You can also set other parameters like organization, timeout, etc. if needed
#     # Replace with the actual base URL for the OpenRouter API
# )

# my_model = OpenAIChatCompletionsModel(
#     model="openai/gpt-3.5-turbo",
#     openai_client=client,
# )


# # Disable tracing for the agent
# set_tracing_disabled(True)  # Uncomment this line if you want to disable tracing


# # Define the agent
# agent = Agent(
#     name="Teacher",
#     instructions="Answer the question and perform the task and" \
#                   "First answer in detailed English, then give a short beautiful Urdu explanation..",
#     model=my_model,
# )

# # response = Runner.run_sync(
# #     starting_agent=agent,
# #     input=input("Enter your question: ===> Q:"),  # Get user input for the question
# # #)

# @cl.on_message
# async def main(message: cl.Message):

#     response = Runner.run_sync(
#         starting_agent=agent,
#         input=message.content,  # Get user input for the question
#     )

#     await cl.Message(
#         content= f"AI Agent ğŸ¤–: {response.final_output}",).send()
 
#______________________________________________________________________
# # @cl.on_message
# # async def main(message: cl.Message):
   
# #     response = Runner.run_sync(starting_agent=agent, input=message.content)
# #     await cl.Message(content=response.final_output,).send() 

# # # Print final output
# # # print("\nğŸ“š Final Response:\n")
# # # print(main.final_output)
#=====================================================================================================


#               =====================
#"""                 ğŸ“Œ  Step 2ï¸                            """
#               =====================

# from litellm import completion
# import os
# from dotenv import load_dotenv

# # Load API Key                     #$env:PYTHONUTF8=1 >> python main.py

# load_dotenv()

# SYED_API_KEY =  os.getenv("GEMINI_API_KEY")

# messages = [{"role": "user", "content": "Hello, how are you?"}]

# response = completion(model="gemini/gemini-2.0-flash-exp", messages=messages)

# print(response['choices'][0]['message']['content'])

# print(response)

#__________________________________________________________________________

# Call Gemini model via LiteLLM
# response = completion(
#     model="gemini/gemini-pro",
#     messages=[{"role": "user", "content": "Hello Gemini, what can you do?"}]
# )

# Print response
#print("Gemini:", response['choices'][0]['message']['content'])
#print(response)

###___________________________________________________________________________


# #Call OpenRouter model via LiteLLM
# def syed(user_input):
#     result = completion(
#         model="openrouter/gemini-2.0-flash-exp",
#         messages=[{"role": "user", "content": user_input}]
#     )
#     return (result['choices'][0]['message']['content'])


#=====================================================================================================

#               =====================
#"""                 ğŸ“Œ  Step 3ï¸âƒ£                             """
#               =====================

# #============== In this we are converting from one file to the other file ================

# # LiteLLM Ø³Û’ completion function Ø§Ù…Ù¾ÙˆØ±Ù¹ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº
# from litellm import completion

# # Ø³Ø³Ù¹Ù… Ø³Û’ environment variables Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ os Ù…Ø§ÚˆÛŒÙˆÙ„
# import os

# # .env ÙØ§Ø¦Ù„ Ø³Û’ API keys ÙˆØºÛŒØ±Û Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’
# from dotenv import load_dotenv

# # .env ÙØ§Ø¦Ù„ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº ØªØ§Ú©Û ÛÙ… Ø§Ù¾Ù†ÛŒ API key Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ÛŒÚº
# load_dotenv()

# # Ø§Ù¾Ù†ÛŒ Gemini API Key .env ÙØ§Ø¦Ù„ Ø³Û’ Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº
# SYED_API_KEY = os.getenv("GEMINI_API_KEY")

# def syed(user_input):
#     # LiteLLM Ú©Û’ completion function Ú©Ùˆ Ú©Ø§Ù„ Ú©Ø±ÛŒÚº
#     answer = completion(
#         model="gemini/gemini-2.0-flash-exp",  # Ù…Ø§ÚˆÙ„ Ú©Ø§ Ù†Ø§Ù…
#         messages=[{"role": "user", "content": user_input}] , # ÛŒÙˆØ²Ø± Ú©Ø§ Ù¾ÛŒØºØ§Ù…
#         stream=False                    # ÛŒÛØ§Úº stream=False ÛÛ’ ØªØ§Ú©Û Ù…Ú©Ù…Ù„ Ø¬ÙˆØ§Ø¨ Ø§ÛŒÚ© Ø¨Ø§Ø± Ù…ÛŒÚº Ù…Ù„ Ø³Ú©Û’
#     )
#     return (answer['choices'][0]['message']['content'])  # type: ignore # AI Ú©Ø§ Ø¬ÙˆØ§Ø¨ ÙˆØ§Ù¾Ø³ Ú©Ø±ÛŒÚº

# # response = syed("Hello, how are you?")
# # print(response)

#_____________________________________________________________________________________
    
# # ÛŒÙˆØ²Ø± Ú©Ø§ Ù¾ÛŒØºØ§Ù… (Ù¾Ø±ÙˆÙ…Ù¾Ù¹) Ø¬Ùˆ ÛÙ… AI Ú©Ùˆ Ø¨Ú¾ÛŒØ¬Ù†Ø§ Ú†Ø§ÛØªÛ’ ÛÛŒÚº
# messages = [{"role": "user", "content": "Hello, how are you?"}]

# # LiteLLM Ú©Ùˆ Ú©Ø§Ù„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº â€” ÛŒÛØ§Úº stream=False ØªØ§Ú©Û ÛÙ…ÛŒÚº Ù…Ú©Ù…Ù„ Ø±ÛŒØ³Ù¾Ø§Ù†Ø³ Ù…Ù„Û’
# response = completion(
#     model="gemini/gemini-2.0-flash-exp",  # Ù…Ø§ÚˆÙ„ Ù†ÛŒÙ…
#     messages=messages,
#     stream=False  # streaming Ø¨Ù†Ø¯ ÛÛ’ ØªØ§Ú©Û ÛÙ… dictionary Ø¬ÛŒØ³Ø§ Ø±Ø³Ù¾Ø§Ù†Ø³ Ù„Û’ Ø³Ú©ÛŒÚº
# )

# # AI Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ù¾Ø±Ù†Ù¹ Ú©Ø±ÛŒÚº
# print(response['choices'][0]['message']['content'])

# # Ù¾ÙˆØ±Ø§ Ø±Ø³Ù¾Ø§Ù†Ø³ (debug ÛŒØ§ Ù…Ú©Ù…Ù„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Û’ Ù„ÛŒÛ’)
# #print(response)

#=====================================================================================================

# #               =====================
# """                 ğŸ“Œ  Step 4ï¸âƒ£                             """
# #               =====================


# from agents import Agent , Runner ,OpenAIChatCompletionsModel, set_tracing_disabled #AsyncOpenAI
# from openai import AsyncOpenAI
# from dotenv import load_dotenv
# import os
# import asyncio

# load_dotenv()

# set_tracing_disabled(True)

# provider = AsyncOpenAI(
#         api_key = os.getenv("GEMINI_API_KEY"),
#         base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
#     )

# # creating model
# model = OpenAIChatCompletionsModel(
#     model = "gemini-2.0-flash-exp",
#     openai_client = provider

# ) 

# # creating agent
# MyAgent = Agent(
#     name = "Assistant",
#     instructions= "You will response to user query",
#     model = model
# )

# #running agent
# async def main():
#     response  = await Runner.run(starting_agent = MyAgent, input = "Hello, how are you?",)
#     print(response.final_output)


# asyncio.run(main())

# # response  = Runner.run_sync(
# #     starting_agent = MyAgent,
# #     input = "Hello, how are you?",
# # )

# # print(response.final_output)
















































# from litellm import completion
# import os
# from dotenv import load_dotenv


# # Load environment variables from .env file
# load_dotenv()

# response = completion(
#   model="nvidia/llama-3.3-nemotron-super-49b-v1:free",
#   messages=[{ "content": "Hello, how are you?","role": "user"}]
# )
# #print(response["choices"][0]["message"]["content"])
# print(response)

































