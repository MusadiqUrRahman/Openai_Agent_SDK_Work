
# #               =====================
# """                 ğŸ“Œ  Step 1ï¸âƒ£                             """
# #               =====================

from agents import Agent , Runner ,OpenAIChatCompletionsModel, set_tracing_disabled #AsyncOpenAI
from openai import AsyncOpenAI
from dotenv import load_dotenv
import os
#import asyncio

load_dotenv()

set_tracing_disabled(True)

provider = AsyncOpenAI(
        api_key = os.getenv("GEMINI_API_KEY"),
        base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
    )

# creating model
model = OpenAIChatCompletionsModel(
    model = "gemini-2.0-flash-exp",
    openai_client = provider

) 

# creating agent
MyAgent = Agent(
    name = "Assistant",
    instructions= "You will response to user query",
    model = model
)

#running agent
##Asynchronous Code? 

async def musadiq(user_query):
    response  = await Runner.run(starting_agent = MyAgent, input = user_query,)
    return response.final_output
    #print(response.final_output)

#asyncio.run(musadiq("Hello, how are you?"))
#asyncio.run(musadiq(input("Enter your question: ===> Q:")))

#_______________________________________________________-
## Synchronous Code?

# def rahman(user_query):
#     response  = Runner.run_sync(
#         starting_agent = MyAgent,
#         input = user_query,
#     )
#     print(response.final_output)

# print(rahman("Hello, how are you?"))


#====================================================================================================

# #               =====================
# """                 ğŸ“Œ  Step 3ï¸âƒ£                             """
# #               =====================

# #============== In this we are converting from one file to the other file ================

# # LiteLLM Ø³Û’ completion function Ø§Ù…Ù¾ÙˆØ±Ù¹ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº
# from litellm import completion

# # Ø³Ø³Ù¹Ù… Ø³Û’ environment variables Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ os Ù…Ø§ÚˆÛŒÙˆÙ„
# import os

# # .env ÙØ§Ø¦Ù„ Ø³Û’ API keys ÙˆØºÛŒØ±Û Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’
# from dotenv import load_dotenv

# # .env ÙØ§Ø¦Ù„ Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº ØªØ§Ú©Û ÛÙ… Ø§Ù¾Ù†ÛŒ API key Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø± Ø³Ú©ÛŒÚº
# load_dotenv()

# # Ø§Ù¾Ù†ÛŒ Gemini API Key .env ÙØ§Ø¦Ù„ Ø³Û’ Ø­Ø§ØµÙ„ Ú©Ø±ÛŒÚº
# SYED_API_KEY = os.getenv("GEMINI_API_KEY")

# def syed(user_input):
#     # LiteLLM Ú©Û’ completion function Ú©Ùˆ Ú©Ø§Ù„ Ú©Ø±ÛŒÚº
#     answer = completion(
#         model="gemini/gemini-2.0-flash-exp",  # Ù…Ø§ÚˆÙ„ Ú©Ø§ Ù†Ø§Ù…
#         messages=[{"role": "user", "content": user_input}] , # ÛŒÙˆØ²Ø± Ú©Ø§ Ù¾ÛŒØºØ§Ù…
#         stream=False                    # ÛŒÛØ§Úº stream=False ÛÛ’ ØªØ§Ú©Û Ù…Ú©Ù…Ù„ Ø¬ÙˆØ§Ø¨ Ø§ÛŒÚ© Ø¨Ø§Ø± Ù…ÛŒÚº Ù…Ù„ Ø³Ú©Û’
#     )
#     return (answer['choices'][0]['message']['content'])  # type: ignore # AI Ú©Ø§ Ø¬ÙˆØ§Ø¨ ÙˆØ§Ù¾Ø³ Ú©Ø±ÛŒÚº

# # response = syed("Hello, how are you?")
# # print(response)
